title
Faster Than Real-time Facial Alignment: A 3D Spatial Transformer Network Approach in Unconstrained Poses
abstract
Facial alignment involves finding a set of landmark points on an image with a known semantic meaning. However, this semantic meaning of landmark points is often lost in 2D approaches where landmarks are either moved to visible boundaries or ignored as the pose of the face changes. In order to extract consistent alignment points across large poses, the 3D structure of the face must be considered in the alignment step. However, extracting a 3D structure from a single 2D image usually requires alignment in the first place. We present our novel approach to simultaneously extract the 3D shape of the face and the semantically consistent 2D alignment through a 3D Spatial Transformer Network (3DSTN) to model both the camera projection matrix and the warping parameters of a 3D model. By utilizing a generic 3D model and a Thin Plate Spline (TPS) warping function, we are able to generate subject specific 3D shapes without the need fora large 3D shape basis. In addition, our proposed network can be trained in an end-to-end framework on entirely synthetic data from the 300W-LP dataset. Unlike other 3D methods, our approach only requires one pass through the network resulting in a faster than realtime alignment. Evaluations of our model on the Annotated Facial Landmarks in the Wild (AFLW) and AFLW2000-3D datasets show our method achieves state-of-the-art performance over other 3D approaches to alignment.
Introduction
Robust face recognition and analysis are contingent upon accurate localization of facial features. When modeling faces, the landmark points of interest consist of points that lie along the shape boundaries of facial features, e.g. eyes, lips, mouth, etc. When dealing with face images collected in the wild conditions, facial occlusion of landmarks becomes a common problem for off-angle faces. Predicting the occlusion state of each landmarking points is one of the. A subject from the CMU Multi-PIE dataset landmarked and frontalized by our method at various poses. Landmarks found by our model are overlaid in green if they are determined to be a visible landmark and blue if self-occluded. The non-visible regions of the face are determined by the estimated camera center and the estimated 3D shape. Best viewed in color.
challenges due to variations of objects in faces, e.g. beards and mustaches, sunglasses and other noisy objects. Additionally, face images of interest nowadays usually contain off-angle poses, illumination variations, low resolutions, and partial occlusions.
Many complex factors could affect the appearance of a face image in real-world scenarios and providing tolerance to these factors is the main challenge for researchers. Among these factors, pose is often the most important factor to be dealt with. It is known that as facial pose deviates from a frontal view, most face recognition systems have difficulty in performing robustly. In order to handle a wide range of pose changes, it becomes necessary to utilize 3D structural information of faces. However, many of the existing 3D face modeling schemes have many drawbacks, such as computation time and complexity. Though these can be mitigated by using depth sensors or by tracking results from frame to frame in video, this can cause difficulty when they have to be applied in real-world large scale unconstrained face recognition scenarios where video and depth information is not available. The 3D generic elastic model (3D-GEM) approach was proposed as an efficient and reliable 3D modeling method from a single 2D image. Heo et al. claim that the depth information of a face is not extremely discriminative when factoring out the 2D spatial location of facial features. In our method, we follow this idea and observe that fairly accurate 3D models can be generated by using a simple mean shape deformed to the input image at a relatively low computational cost compared to other approaches. We take the approach of using a simple mean shape and using a parametric, non-linear warping of that shape through alignment on the image to be able to model any unseen example. A key flaw in many approaches that rely on a 3D Morphable Model (3DMM) is that it needs enough examples of the data to be able to model unseen samples. However, in the case of 3D faces, most datasets are very small.

Our Contributions in this Work
(2) Our approach is efficiently implemented in an endto-end deep learning framework allowing for the alignment and 3D modeling tasks to be codependent. This ensures that alignment points are semantically consistent across changing poses of the object which also allows for more consistent 3D model generation and frontalization on images in the wild as shown in Figs. 1 and 2.
(3) Our method only requires a single pass through the network allowing us to achieve faster than real-time processing of images with state-of-the-art performance over other 2D and 3D approaches to alignment.

Related Work
There have been numerous studies related to face alignment since the first work of Active Shape Models (ASM) in 1995. A comprehensive literature review in face alignment is beyond the scope of this work. In this paper, we mainly focus on recent Convolutional Neural Network (CNN) approaches to solve the face alignment problem. Especially those methods aimed at using 3D approaches to achieve robust alignment results.

Face Alignment Methods
While Principal Component Analysis (PCA) and its variants were successfully used to model the facial shapes and appearances, there have since been many advances in facial alignment. Landmark locations can be directly predicted by a regression from a learned feature space. Xiong et al. presented the Global Supervised Descent Method (GSDM) method to solve the problem of 2D face alignment. The objective function in GSDM is divided into multiple regions of similar gradient directions. It then constructs a separate cascaded shape regressor for each region. Yu et al. incorporated 3D pose landmarking models with group sparsity to indicate the best landmarks. These kind of methods shows an increase of performance on landmark localization. However, these methods all rely on hand-crafted features. Recently, CNN-based methods have achieved good results in facial alignment. 3DDFA fits a dense 3D face model to the image via CNN and DDN proposes a novel cascaded framework incorporating geometric constraints for localizing landmarks in faces and other non-rigid objects. Recently, shape regression has been used in numerous facial landmarking methods.
There are several recent works studying the human head rotations, nonlinear statistical models () and 3D shape models. Nonlinear statistical model approaches are impractical in real-time applications. Viewbased methods employ a separate model for each viewpoint mode. Traditionally, the modes are specified as part of the algorithm design, and problems can arise at midpoints between models.

CNNs for 3D Object Modeling
While estimating a 3D model from images is not anew problem, the challenging task of modeling objects from a single image has always posed a challenge. This is, of course, due to the ambiguous nature of images where depth information is removed. With the recent success of deep learning and especially CNNs in extracting salient information from images, there have been many explorations into how to best use CNNs for modeling objects in 3 dimensions. Many of these approaches are aimed creating a depth estimation for natural images. While the results on uncontrolled images are impressive, the fact that these models are very general means they tend to suffer when applied to specific objects, such as faces. In fact, many times, the depth estimate for faces in the scene tend to be fairly flat. By limiting the scope of the method, the resulting estimated 3D model can be made much more accurate. Hassner et al. use a 3D model of the face to be able to frontalize faces in unseen images with the end goal of improving face recognition by limiting the variations the matcher has to learn. However, this approach requires landmarks on the input face in the same fashion as other methods.
A 2D approach to landmarking inevitably suffers from the problem of visibility and self-occlusion. As Zhu et al. show, the problem of landmark marching, where landmarks tend to move to the visible boundary, can cause issues when estimating 3D models from purely 2D alignment. However, this problem can be alleviated by using a 3D model of the face in the alignment step itself as done in. Both of these methods make use of an underlying 3D Morphable Model (3DMM) and try to fit the model to the input image in order to find the 2D landmarks. This of course requires a basis to use and the Basel Face Model (BFM) is a very popular model to use. However, the BFM is only created from a set of 100 male and 100 female scans. As any basis can only recreate combinations of the underlying samples, this can severely limit the capability of these models to fit outlier faces or expressions not seen before. Although there has been recent efforts to generate more accurate 3DMMs, neither the data nor the model is available to researchers in the field of biometrics. Therefore, we propose to use a smooth warping function, Thin Plate Splines (TPS), to warp mean shapes to fit the input image and generate new 3D shapes. In this fashion, any new face can be modeled, even if its shape cannot be reconstructed by the BFM.

3D Spatial Transformer Networks
In order to model how a face truly changes from viewpoint to viewpoint, it is necessary to have both the true 3D model of the subject in the image and the properties of the camera used to capture the image, usually in the form of the camera projection matrix. However, knowledge of the true 3D model and the camera projection matrix are almost always not available. Jaderberg et al., in their work on Spatial Transformer Networks, use a deep network to estimate the parameters of either an affine transformation or a 2D Thin Plate Spline (TPS) transformation. These parameters are then used to generate anew sampling grid which can then be used to generate the transformed image.
We approach finding the unknown camera projection matrix parameters and the parameters needed to generate the 3D model of the head in a similar fashion. Both the camera projection parameters and the warping parameters, a TPS warp in this case, can be estimated from deep features generated from the image using any architecture. The TPS parameters can be used to warp a model of the face to match what the network estimates the true 3D shape is and the camera projection parameters can be used to texture the 3D coordinates from the 2D image. Additionally, the pose of the face can be determined from the camera parameters allowing fora visibility map to be generated for the 3D model. This allows us to only texture vertexes that are visible in the image as opposed to vertexes that are occluded by the face itself. The architecture of our model is shown in. Sections 3.1,3.2, and 3.3 detail how to create differentiable modules to utilize the camera projection and TPS parameters that are estimated by the deep network to warp and project a 3D model to a 2D image plane for texture sampling.

Camera Projection Transformers
In order to be able to perform end-to-end training of a network designed to model 3D transformations of the face, a differentiable module that performs a camera projection must be created. This will be part of the grid generator portion of the Spatial Transformer. Modeling how a 3D point will map to the camera coordinates is expressed by the well known camera projection equation
where p c is the homogeneous 2D point in the camera coordinate system, p w is the homogeneous 3D point in the world coordinate system, and M is the 3x4 camera projection matrix. This relationship is only defined up to scale due to the ambiguity of scale present in projective geometry, hence the = instead of a hard equality. The camera projection matrix has only 11 degrees of freedom since it is only defined up to scale as well. Therefore, this module takes in the 11 parameters estimated by a previous layer as the input in the form of a length 11 vector, a. In order to perform backpropogation on the new grid generator, the derivative of the generated grid with respect to a must be computed.
Since Eqn. 1 is only defined up to scale, the final output of this module will have to divide out the scale factor. By first rewriting the camera projection matrix as M = ? ? a 1 a 2 a 3 a 4 a 5 a 6 a 7 a 8 a 9 a 10 a 11 1
where a i is the i th element of a, the final output of the camera projection module can be written as
The gradient with respect to each of the rows of M can be shown to be
Using the chain rule, the gradient of the loss of the network with respect to the input can be found as
Since M is only defined up to scale, the last element of M can be defined to be a constant which means that only the first 11 elements of this gradient are used to actually perform the backpropogation on a. Since M relates many pairs of 2D and 3D points, the gradient is computed for every pair and added together to give the final gradient that is used for updating a.

3D Thin Plate Spline Transformers
When modeling the 3D structure of a face, a generic model cannot represent the variety of shapes that might be seen in an image. Therefore, some method of warping a model must be used to allow the method to handle unseen shapes. Thin Plate Spline (TPS) warping has been used by many applications to great effect. TPS warps have the very dersirable features of providing a closed form of a smooth, parameterized warping given a set of control points and desired destination points. Jaderberg et al. showed how 2D TPS Spatial Transformers could lead to good normalization of nonlinearly transformed input images. Applying a TPS to a 3D set of points follows a very similar process. As in, the TPS parameters would be estimated from a deep network of some sort and passed as input to a 3D grid generator module.
A 3D TPS function is of the form
, and w jx are the parameters of the function, c j is the j th control point used in determining the function parameters, and U (r) = r 2 log r. This function is normally learned by setting up a system of linear equations using the known control points, c j and the corresponding points in the warped 3D object. The function finds the change in a single coordinate, the change in the x-coordinate in the case of Eqn. 6. Similarly, one such function is created for each dimension, i.e. f ?x (x, y, z), f ?y (x, y, z), and f ?z (x, y, z). The 3D TPS module would then take in the parameters for all three of these functions as input and output the newly transformed points on a 3D structure as
This means that the 3D TPS module must have all of the 3D vertices of the generic model and the control points on the generic model as fixed parameters specified from the start. This will allow the module to warp the specified model by the warps specified by the TPS parameters. As in 3.1, the gradient of the loss with respect to the input parameters must be computed in order to perform backpropogation on this module. As usual, the chain rule can be used to find this by computing the gradient of the output with respect to the input parameters. Since each 3D vertex in the generic model will give one 3D vertex as an output, it is easier to compute the gradient on one of these points, pi = (x i , y i , z i ), first. This can be shown to be
where ? ?x are the parameters off ?x . Similarly, the gradients for ? ?y and ? ?z are the same with only the non-zeros values in either the second or third row respectively. The final gradient of the loss with respect to the parameters can be computed as
Since this is only fora single point, once again the gradient can be computed for every point and added for each set of parameters to get the final gradient for each set of parameters that can be used to update previous layers of the network.

Warped Camera Projection Transformers
In order to make use of the TPS warped 3D points in the camera projection module of the transformer network, the module must take in as input the warped coordinates. This means that such a module would also have to do backpropogation on the 3D coordinates as well as the camera projection parameters. Since 3.1 already specified how to compute the gradient of the loss with respect to the camera projection parameters, all that is left to do is compute the gradient of the loss with respect to the 3D coordinates in this module. Taking the derivative of the output in Eqn. 3 with respect to the 3D point, p w results in
However, since p w is in homogeneous coordinates and only the gradient with respect to the x, y, and z coordinates are needed, the actual gradient becomes
where
and m ij is the j th element of mi . This gradient is computed for every 3D point independently and used in the chain rule to compute
which can then be used to perform backpropogation on each p w .

2D Landmark Regression
In order to further improve the landmark accuracy, we extend our network with a landmark refinement stage. This stage treats the projected 2D coordinates from the previous stage as initial points and estimates the offsets for each point. To extract the feature vector for each point, a 3 × 3 convolution layer is attached on top of the last convolution layer in the base model, followed by a 1 × 1 convolution layer for more nonlinearity, resulting in a feature map with D channels. Then each initial point is projected onto this feature map and its D-dimensional feature vector is extracted along the channel direction. Notice that the initial points are often not aligned with the grids on the feature map. Therefore, their feature vectors are sampled with bilinear interpolation.
Given the feature vector for each landmark, it goes through a fully-connected (FC) layer to output the offsets, i.e. ? x and ? y . Then the offsets are added to the coordinates of the initial location. For each landmark we use an independent FC layer. We don't share the FC layer for all landmarks because each landmark should have a unique behavior of offsets. For example, the center of the eye may move left after regression whereas the corner of the eye may move right. Also, sometimes two initial landmarks maybe projected to the same location due to a certain pose. We want them to move to different locations even when they have the same feature vector.

3D Model Regression From 2D Landmarks
Once the 2D regression is performed, the mapping between the 3D model and the 2D landmarks is broken. While this is not necessarily a problem in the case of sparse facial alignment, if a denser scheme is needed, the entire model would have to be retrained. In order to avoid this, we create anew 3D model that does map to these 2D landmarks by finding anew set of 3D coordinates that project to the new 2D landmarks and warping the 3D model to fit these new points. To find the new 3D coordinates, we need to backproject rays through each of the 2D landmarks through 3D space using the camera projection matrix we have estimated. The equation for the ray of points associated with a given homogeneous 2D point, pi 2D , is defined as
where A and bare the first three and the last column of the estimated camera projection matrix respectively. These rays represent all possible points in 3D that could project to the determined locations in the image. We then find the closest point, pi 3D , on the ray to the original 3D coordinate, pi 3D , to use as the new 3D point as shown in. These new correspondences are used to perform a TPS warping of the model. After this warping, the landmark points on the model will project to exactly the regressed 2D landmarks, recovering the mapping between the 3D model and the 2D image. This new model can then be projected onto the image to generate a much more accurate texturing of the 3D model. This same style of warping can be used to move the 3D coordinates anywhere we choose. This means neutralizing out expressions, especially smiles, is very easy to do by using the texture from the regressed 3D shape. While the non-smiling shape will not be as accurate due to the fact that a non-smiling image was not seen, it still gives convincing qualitative results, as seen in, which indicate it maybe a worthwhile avenue of exploration for future work, especially in face recognition.

Experiments

Datasets

300W-LP:
The 300W-LP dataset contains 122,450 synthetically generated views of faces from the AFW, LFPW, HELEN, and IBUG datasets. These images not only contain rotated faces but also attempt to move the background in a convincing fashion, making it a  The resulting regressed 3D model (green box) maintains the smile and is very similar to the input image while the same texture applied to the original shape (red box) suffers a small degradation in shape but allows fora non-smiling rendering of the input image. very useful dataset for training 3D approaches to work on real world images.
AFLW: The Annotated Facial Landmarks in the Wild (AFLW) dataset is a relatively large dataset for evaluating facial alignment on wild images. It contains approximately 25,000 faces annotated with 21 landmarks with visibility labels. The dataset provides pose estimates so results are grouped into three different pose ranges, [0 • , 30 • ], (30 • , 60 • ], and (60 • , 90 • ]. Due to the inconsistency in the bounding boxes in the AFLW dataset, we adopt the use of a face detector first to normalize the scale of the faces. The Multiple Scale Faster Region-based CNN approach has shown good results and at a fast speed. We use the recent extension to this work, the Contextual Multi-Scale Region-based CNN (CMS-RCNN) approach to perform the face detection in any experiment where face detection is needed. The CMS-RCNN approach detects 98.8% (13,865), 95.9% (5,710), and 86.5% of the faces in the [0 • , 30 • ], (30 • , 60 • ], and (60 • , 90 • ] pose ranges respectively.
AFLW2000-3D: Zhu et al. accurately pointed out how merely evaluating an alignment scheme on the visible landmarks in a dataset can result in artificially low errors. Therefore, a true evaluation of any 3D alignment method must also evaluate alignment on the non-visible landmarks as well. The AFLW2000-3D dataset contains the first 2000 images of the AFLW dataset but with all 68 points defined by the scheme in the CMU MPIE dataset. These points were found by aligning the Basel Face Model to the images. While this is a synthetic dataset, meaning the true location of the non-visible landmarks is not known, it is the best one can do when dealing with real images. As these images are from the AFLW dataset, they are also grouped into the same pose ranges.

Implementation Details
Our network is implemented in the Caffe framework. A new layer is created consisting of the 3D TPS transformation module, the camera projection module and the bilinear sampler module. All modules are differentiable so that the whole network can be trained end-to-end.
We adopt two architectures, AlexNet and VGG-16, as the pre-trained models for our shared feature extraction networks in, i.e. we use the convolution layers from the pre-trained models to initialize ours. Since these networks already extract informative low-level features and we do not want to lose this information, we freeze some of the earlier convolution layers and finetune the rest. For the AlexNet architecture, we freeze the first layer while for the VGG-16 architecture, the first 4 layers are frozen.
The 2D landmark regression is implemented by attaching additional layers on top of the last convolution layer. With N landmarks to regress, we need NFC layers to compute the offsets for each individual landmark. While it's possible to setup N individual FC layers, here we implement this by adding one Scaling layer followed by a Reduction layer and Bias layer. During training only the new layers are updated and all previous layers are frozen.

Training on 300W-LP
When training our model, we train on the AFW, HE-LEN, and LFPW subsets of the 300W-LP dataset and use the IBUG portion as a validation set. All sets are normalized using the bounding boxes from the CMS-RCNN detector by reshaping the detected faces to 250 x 250 pixels. For the AlexNet architecture, we train for 100,000 iterations with a batch size of 50. The initial learning rate is set to 0.001 and drops by a factor of 2 after 50,000 iterations. When training the landmark regression, the initial learning rate is 0.01 and drops by a factor of 10 every 40,000 iterations. For the VGG-16 architecture, we train for 200,000 iterations with a batch size of 25. The initial learning rate is set to 0.001 and drops by a factor of 2 after 100,000 iterations. When training the landmark regression, the initial learning rate is 0.01 and drops by a factor of 10 every 70,000 iterations. The momentum for all experiments is set to 0.9. Euclidean loss is applied to 3D vertexes, 2D projected landmarks and 2D

Ablation Experiments
To investigate the effect of each component in our network, we conduct two ablation studies. All the models in these experiments are trained on the same 300W LP dataset and tested on the detected images in AFLW. We first test the effect of the different pre-trained models. We fine-tune our network from the AlexNet and VGG-16 models pretrained on the ImageNet dataset and evaluate the landmark accuracy before the regression step. The VGG-16 model outperforms the AlexNet model in all three pose ranges on the AFLW detected set as shown in. This seems to indicate that a good base model is important for the parameter estimation portion of the network. Second, we evaluate the effect of landmark regression stage. We compare the errors between the regressed and projected landmarks. shows that the landmark regression step greatly helps to improve the accuracy.

Comparison Experiments
AFLW: Since the CMS-RCNN approach may only detect the easier to landmark faces, we use the provided bounding box anytime the face is not detected by the detector. Due to the inconsistency between the two bounding box schemes, faces are not always normalized properly. However, we feel this is the only way to get a fair comparison to other methods without artificially making the dataset easier by only evaluating on detected faces. We compare against baseline methods used by on the same dataset, namely Cascaded Deformable Shape Models (CDM), Robust Cascaded Pose Regression (RCPR), Explicit Shape Regression (ESR), SDM and 3DDFA. All methods except for CDM were retrained on the 300W-LP dataset. The Normalized Mean Error (NME) is computed by averaging the error of the visible landmarks and normalizing it by the square root of the bounding box size (h x w) provided in the dataset. clearly shows that our model using the VGG-16 architecture has achieved better accuracy in all pose ranges, especially the (60 • , 90 • ] category, and has achieved a smaller standard deviation in the error. This means that not only are the landmarks more accurate, they are more consistent than the other methods.. CED curves for both the AlexNet (red) and VGG-16 (green) architectures on both the AFLW (left) and AFLW2000-3D (right) dataset. To balance the distributions, we randomly sample 13,209 faces from AFLW and 915 faces from AFLW2000-3D, split evenly among the 3 categories, and compute the CED curve. This is done 10 times and the average of the resulting CED curves are reported. The mean NME% for each architecture from is also reported in the legend. AFLW2000-3D: The baseline methods were evaluated using the bounding box of the 68 landmarks so we retrained our models using the same bounding box on the training data. Generating these is trivial due to the 3D models. The NME is computed using the bounding box size. Here we see that though 3DDFA+SDM performs well, the VGG-16 architecture of our model still performs best in both the [0 • , 30 • ] and (60 • , 90 • ] ranges. While the VGG-16 model is only second best in the (30 • , 60 • ] range by a small amount, the improvement in (60 • , 90 • ] means that, once again, our method generates more accurate and more consistent landmarks, even in a 3D sense. Cumulative Error Distribution (CED) curves are reported for both architectures on both datasets in.

Running Speed
In order to evaluate the speed of our method, we evaluate the models on a random subset of 1200 faces from the AFLW subset split evenly into the [0 • , 30 • ], (30 • , 60 • ], and (60 • , 90 • ] pose ranges. The images are processed one at a time to avoid any benefit from batch processing. The models are evaluated on a 3.40 GHz Intel Core i7-6700 CPU and an NVIDIA GeForce GTX TITAN X GPU. Our AlexNet trained model takes a total of 7.064 seconds to landmark the 1200 faces for an average of 0.0059 seconds per image or approximately 170 faces per second. The deeper and more accurate VGG-16 model landmarks the 1200 faces in 22.765 seconds for an average of 0.0190 seconds or approximately 52 faces per second. In comparison, the 3DDFA approach takes 75.72 ms (3 iterations at 25.24 ms per iteration as specified in) with 2/3 of the time being used to process data on the CPU.

Conclusions
In this paper we propose a method using 3D Spatial Transformer Networks with TPS warping to generate both a 3D model of the face and accurate 2D landmarks across large pose variation. The limited data used in the generation of a 3DMM can mean that unseen face shapes cannot be modeled. By using a TPS warp, any potential face can be modeled through a regression of 2D landmarks, of which there is much more data available. We have shown how this approach leads to more accurate and consistent landmarks over other 2D and 3D methods.