title
0,000 + Times Accelerated Robust Subset Selection ( ARSS )
abstract
Subset selection from massive data with noised information is increasingly popular for various applications .
This problem is still highly challenging as current methods are generally slow in speed and sensitive to outliers .
To address the above two issues , we propose an accelerated robust subset selection ( ARSS ) method .
Specifically in the subset selection area , this is the first attempt to employ the p ( 0 < p ? 1 ) - norm based measure for the representation loss , preventing large errors from dominating our objective .
As a result , the robustness against outlier elements is greatly enhanced .
Actually , data size is generally much larger than feature length , i.e. N L. Based on this observation , we propose a speedup solver ( via ALM and equivalent derivations ) to highly reduce the computational cost , theoretically from ON 4 to ON 2 L .
Extensive experiments on ten benchmark datasets verify that our method not only outperforms state of the art methods , but also runs 10,000 + times faster than the most related method .
Introduction
Due to the explosive growth of data , subset selection methods are increasingly popular fora wide range of machine learning and computer vision applications .
This kind of methods offer the potential to select a few highly representative samples or exemplars to describe the entire dataset .
By analyzing a few , we can roughly know all .
Such case is very important to summarize and visualize huge datasets of texts , images and videos etc . .
Besides , by only using the selected exemplars for succeeding tasks , the cost of memories and computational time will be greatly reduced .
Additionally , as outliers are generally less representative , the side effect of outliers will be reduced , thus boosting the performance of subsequent applications .
There have been several subset selection methods .
The most intuitional method is to randomly select a fixed number of samples .
Although highly efficient , there is no guarantee for an effective selection .
For the other methods , depending on the mechanism of representative exemplars , there are mainly three categories of selection methods .
One category Data size ( N ) Selection Time
Classifiers
Classification Accuracy Performance TED RRSSNie RRSSour ARSSour : Comparisons of four algorithms on Optdigit .
Two conclusions can be drawn .
First , our method ( ARSSour ) is highly faster than all others ; with the help of an elegant new theorem , RRSSour is significantly faster than the authorial algorithm RRSSNie.
Second , ARSSour achieves highly promising prediction accuracies .
relies on the assumption that the data points lie in one or multiple low - dimensional subspaces .
Specifically , the Rank Revealing QR ( RRQR ) selects the subsets that give the best conditional sub-matrix .
Unfortunately , this method has suboptimal properties , as it is not assured to find the globally optimum in polynomial time .
Another category assumes that the samples are distributed around centers .
The center or its nearest neighbour are selected as exemplars .
Perhaps , Kmeans and Kmedoids are the most typical methods ( Kmedoids is a variant of Kmeans ) .
Both methods employ an EM - like algorithm .
Thus , the results depend tightly on the initialization , and they are highly unstable for large K ( i.e. the number of centers or selected samples ) .
Recently , there area few methods that assume exemplars are the samples that can best represent the whole dataset .
However , for , the optimization is a combinatorial problem ( NP - hard ) , which is computationally intractable to solve .
Besides , the representation loss is measured by the least square measure , which is sensitive to outliers in data .
Then improves by employing a robust loss via the 2 , 1 - norm ; the 1 - norm is applied to samples , and the 2 - norm is used for features .
In this way , the side effect of outlier samples is relieved .
The solver of ) is theoretically perfect due to its ability of convergence to global optima .
Unfortunately , in terms of computational costs , the solver is highly complex .
It takes ON 4 for one iteration as shown in .
This is infeasible for the case of large N ( e.g. it takes 2000 + hours fora case of N = 13000 ) .
Moreover , the representation loss is only robust against outlier samples .
Such case is worth improvement , as there may exist outlier elements in real data .
Contributions .
In this paper , we propose an accelerated robust subset selection method to highly raise the speed on the one hand , and to boost the robustness on the other .
To this end , we use the p ( 0 < p ? 1 ) - norm based robust measure for the representation loss , preventing large errors from dominating our objective .
As a result , the robustness against outliers is greatly boosted .
Then , based on the observation that data size is generally much larger than feature length , i.e. N L , we propose a speedup solver .
The main acceleration is owing to the Augmented Lagrange Multiplier ( ALM ) and an equivalent derivation .
Via them , we reduce the computational complexity from ON 4 to ON 2 L .
Extensive results on ten benchmark datasets demonstrate that in average , our method is 10,000 + times faster than Nie 's method .
The selection quality is highly encouraging as shown in .
Additionally , via another equivalent derivation , we give an accelerated solver for Nie 's method , theoretically reducing the computational complexity from ON 4 to ON 2 L + N L 3 as listed in , empirically obtaining a 500 + times speedup compared with the authorial solver .
Notations .
We use boldface uppercase letters to denote matrices and boldface lowercase letters to represent vectors .
For a matrix Y = [ Y ln ] ?
R LN , we denote it s l throw and n th column as y land y n respectively .
The 2 ,1 - norm of a matrix is defined as
Subset Selection via Self - Representation
In the problem of subset selection , we are often given a set of N unlabelled points X = x 1 , x 2 , , x N | x n ?
R L , where L is the feature length .
The goal is to select the top K ( K N ) most representative and informative samples ( i.e. exemplars ) to effectively describe the entire dataset X .
By solely using these K exemplars for subsequent tasks , we could greatly reduce the computational costs and largely alleviate the side effects of outlier elements in data .
Such a motivation could be formulated as the Transductive Experimental Design ( TED ) model :
where Q ?
R LK is the selected subset matrix , whose column vectors all come from X , i.e. q k ?
X , ?k ?
{ 1 , , K} ; A = [ a 1 , , a N ] ?
R KN is the corresponding linear combination coefficients .
By minimizing ( 1 ) , TED could select the highly informative and representative samples , as they have to well represent all the samples in X .
Although TED ( 1 ) is well modeled - very accurate and intuitive , there are two bottlenecks .
First , the objective is a combinatorial optimization problem .
It is NP - hard to exhaustively search the optimal subset Q from X .
For this reason , the author approximate ( 1 ) via a sequential optimization problem , which is solved by an inefficient greedy optimization algorithm .
Second , similar to the existing least square loss based models in machine learning and statistics , ( 1 ) is sensitive to the presence of outliers
where ?
is a nonnegative parameter ; A is constrained to be row - sparse , and thus to select the most representative and informative samples .
As the representation loss is accumulated via the 1 - norm among samples , compared with ( 1 ) , the robustness against outlier samples is enhanced .
Equivalently , ( 2 ) is rewritten in the matrix format :
Since the objective is convex in A , the global minimum maybe found by differentiating ( 3 ) and setting the derivative to zero , resulting in a linear system 1
where V ?
RN
N is a diagonal matrix with then th diagonal entry as V nn = 1 2 an 2 and U nn = 1 2 xn ?
Xan 2 .
It seems perfect to use ( 4 ) to solve the objective ( 3 ) , because ( 4 ) looks simple and the global optimum is theoretically guaranteed .
Unfortunately , in terms of speed , ( 4 ) is usually infeasible due to the incredible computational demand in the case of large N ( the number of samples ) .
At each iteration , the computational complexity of ( 4 ) is up to ON 4 , as analyzed in Remark 1 .
According to our experiments , the time cost is up to 2088 hours ( i.e. 87 days ) fora subset selection problem of 13000 samples .
Remark
1 . Since U nn X T X +? V ?
RN N , the major computational cost of ( 4 ) focuses on a N N linear system .
If solved by the Cholesky factorization method , it costs 1 3 N 3 for factorization as well as 2N 2 for forward and backward substitution .
This amounts to ON 3 in total .
By now , we only solve an .
Once solving all the set of {a n } N n= 1 , the total complexity amounts to ON 4 for one iteration step .
Accelerated Robust Subset Selection ( ARSS )
Due to the huge computational costs , Nie 's method is infeasible for the case of large N - the computational time is up to 2088 hours fora case of 13000 samples .
Besides , Nie 's model imposes the 2 - norm among features , which is prone to outliers in features .
To tackle the above two issues , we propose a more robust model in the p ( 0 < p ? 1 ) norm .
Although the resulted objective is challenging to solve , a speedup algorithm is proposed to dramatically save the computational costs .
For the same task of N = 13000 , it costs our method 1.8 minutes , achieving a 68429 times acceleration compared with the speed of Nie 's method .
Modeling .
To boost the robustness against outliers in both samples and features , we formulate the discrepancy between X and XA via the p ( 0 < p < 1 ) - norm .
There are theoretical and empirical evidences to verify that compared with 2 or 1 norms , the p - norm is more able to prevent outlier elements from dominating the objective , enhancing the robustness ) .
Thus , we have the following objective min
where ?
is a balancing parameter ; A is a row sparse matrix , used to select the most informative and representative samples .
By minimizing the energy of ( 5 ) , we could capture the most essential properties of the dataset X.
After obtaining the optimal A , the row indexes are sorted by the row - sum value of the absolute A in decreasing order .
The samples specified by the top K indexes are selected as exemplars .
Note that the model ( 5 ) could be applied to the unsupervised feature selection problem by only transposing the data matrix X .
In this case , A is a L L row sparse matrix , used to select the most representative features .
Accelerated Solver for the ARSS Objective in Although objective ( 5 ) is challenging to solve , we propose an effective and highly efficient solver .
The acceleration owes to the ALM and an equivalent derivation .
ALM
The most intractable challenge of ( 5 ) is that , the p ( 0 < p ? 1 ) - norm is non-convex , non-smooth and notdifferentiable at the zero point .
Therefore , it is beneficial to use the Augmented Lagrangian Method ( ALM ) to solve ( 5 ) , resulting in several easily tackled unconstrained subproblems .
By solving them iteratively , the solutions of subproblems could eventually converge to a minimum .
Specifically , we introduce an auxiliary variable E = X ? XA ?
R LN .
Thus , the objective ( 5 ) becomes : min
To deal with the equality constraint in , the most convenient method is to add a penalty , resulting in
where is a penalty parameter .
To guarantee the equality constraint , it requires approaching infinity , which may cause bad numerical conditions .
Instead , once introducing a Lagrangian multiplier , it is no longer requiring ? ?.
Thus , we rewrite into the standard ALM formulation as :
where ?
consists of L N Lagrangian multipliers .
In the following , a highly efficient solver will be given .
The updating rule for ?
Similar to the iterative thresholding ( IT ) in , the degree of violations of the L N equality constraints are used to update the Lagrangian multiplier :
where is a monotonically increasing parameter over iteration steps .
For example , ? ? , where 1 < ? < 2 is a predefined parameter .
Efficient solver for E Removing irrelevant terms with E from ( 8 ) , we have
where H = X ? XA ? ? ?
R LN .
According to the definition of the p - norm and the Frobenius - norm , ( 10 ) could be decoupled into L N independent and unconstrained subproblems .
The standard form of these subproblems is
where ? = 1 is a given positive parameter , y is the scalar variable need to deal with , c is a known scalar constant .
Zuo et al. ) has recently proposed a generalized iterative shrinkage algorithm to solve ( 11 ) .
This algorithm is easy to implement and able to achieve more accurate solutions than current methods .
Thus , we use it for our problem as :
is obtained by solving the following equation :
which could be solved efficiently via an iterative algorithm .
In this manner , ( 10 ) could be sovled extremely fast .
Accelerated solver for A The main acceleration focuses on the solver of A. Removing irrelevant terms with A from ( 8 ) , we have
where ? = ?
is a nonnegative parameter , P = X ? E ? ? ?
R LN . Since ( 13 ) is convex in A , the optimum could be found by differentiating ( 13 ) and setting the derivative to zero .
This amounts to tackling the following linear system 2 :
As V + ? X TX ?
RN N , is mainly a N N linear system .
Once solved by the Cholesky factorization , the computational complexity is highly up to ON 3 .
This is by no means a good choice for real applications with large N .
In the following , an equivalent derivation of ( 14 ) will be proposed to significantly save the computational complexity .
Theorem
2 . The N N linear system is equivalent to the following L L linear system :
where IL is a L L identity matrix .
Proof .
Note that V is a N N diagonal and positive - definite matrix , the exponent of V is efficient to achieve , i.e. V ? = { V ? nn } N n=1 , ?? ?
R.
We have the following equations
where Z = XV ? 1 2 , IN is a N N identity matrix .
The following equation holds for any conditions
Multiplying with IN + ?Z T Z ?1 on the left and IL + ? ZZ T ? 1 on the right of both sides of the equal - sign , we have the equation as :
Therefore , substituting ( 18 ) and Z = XV ? 1 2 into ( 16 ) , we have the simplified updating rule as :
When N L , the most complex operation is the matrix multiplications , not the L L linear system .
Corollary
3 . We have two equivalent updating rules and ( 15 ) for the objective ( 13 ) .
If using ( 14 ) when N ?
L , and otherwise using ( 15 ) as shown in Algorithm 1 , the computational complexity of solvers for ( 13 ) is ON 2 L .
Due to N L , we have highly reduced the complexity from ON 4 to ON 2 L compared with Nie 's method .
Algorithm 1 for ( 13 ) : A * = ARSS A ( X , V , P , IL , ?)
Input : X , V , P , IL , ? 1 : if N ?
L then 2 :
update A via the updating rule , that is 3 : update ? by the updating rule ( 9 ) , ? ?.
8 : until convergence
Output : A The solver to update A is given in Algorithm
1 . The overall solver for our model ( 5 ) is summarized in Algorithm
2 .
According to Theorem 2 and Corollary 3 , the solver for our model ( 13 ) is highly simplified , as feature length is generally much smaller than data size , i.e L N .
Similarly ,
Nie 's method could be highly accelerated by Theorem 4 , obtaining 500 + times speedup , as shown in and .
Theorem
4 . Nie 's N N solver ( 20 ) ) is equivalent to the following L L linear system ( 21 ) an = U nn U nn X TX + ? V
?1 X T x n ( 20 )
?n ? { 1 , 2 , , N } , where IL is a L L identity matrix .
Proof .
Based on ( 20 ) , we have the following equalities :
The derivations are equivalent ; their results are equal .
2 V ?
RN
N is a positive and diagonal matrix with then th diagonal entry as Vnn = 1 ? an 2 2 + > 0 , where is a small value to avoid singular failures Corollary 5 .
Since feature length is generally much smaller than data size , i.e. L N , our accelerated solver ( 20 ) for Nie 's model ( 3 ) is highly faster than the authorial solver ( 21 ) .
Theoretically , we reduce the computational complexity from ON 4 to ON 2 L + N L 3 , while maintaining the same solution .
That is , like Nie 's solver ( 20 ) , our speedup solver ( 21 ) can reach the global optimum .
Extensive empirical results will verify the huge acceleration
Experiments Experimental Settings
In this part , the experimental settings are introduced .
All experiments are conducted on a server with 64 - core Intel Xeon E7-4820 @ 2.00 GHz , 18 Mb Cache and 0.986 TB RAM , using Matlab 2012 .
Brief descriptions often benchmark datasets are summarized in , where ' Total ( N * ) ' denotes the total set of samples in each data .
Due to the high computational complexity , other methods can only handle small datasets ( while our method can handle the total set ) .
Thus , we randomly choose the candidate set from the total set to reduce the sample size , i.e. N < N * ( cf. ' Total ( N * ) ' and ' candid . ( N ) ' in ) .
The remainder ( except candidate set ) are used for test .
Specifically , to simulate the varying quality of samples , ten percentage of candidate samples from each class are randomly selected and arbitrarily added one of the following three kinds of noise : " Gaussian " , " Laplace " and " Salt & pepper " respectively .
Ina word , all experiment settings are same and fair for all the methods .
Speed Comparisons
There are two parts of speed comparisons .
First , how speed varies with increasing N is illustrated in .
Then the comparison of specific speed is summarized in .
Note that TED and RRSS Nie denote the authorial solver ( via authorial codes ) ; RRSS our is our accelerated solver for Nie 's model via Theorem 4 ; ARSS is the proposed method .
Speed vs. increasing N To verify the great superiority of our method over the state - of - the - art methods in speed , three experiments are conducted .
The results are illustrated in , where there are three sub-figures showing the speed of four methods on the benchmark datasets of Letter , MNIST and Waveform respectively .
As we shall see , both selection time of TED and RRSS Nie increases dramatically as N increases .
No surprisingly , RRSS Nie is incredibly time - consuming as N grows the order of curves looks higher than quadratic .
Actually , the theoretical complexity of RRSS Nie is highly up to ON 4 as analyzed in Remark
1 .
Compared with TED and RRSS Nie , the curve of ARSS is surprisingly lower and highly stable against increasing N ; there is almost no rise of selection time over growing N .
This is owing to the speedup techniques of ALM and equivalent derivations .
Via them , we reduce the computational cost from ON 4 to ON 2 L , as analyzed in Theorem 2 and Corollary
3 .
Moreover , with the help of Theorem 4 , RRSS our is the second faster algorithm that is significantly accelerated compared with the authorial algorithm RRSS Nie .
Speed with fixed N The speed of four algorithms is summarized in , where each row shows the results on one dataset and the last row displays the average results .
Four conclusions can be drawn from .
First , ARSS is the fastest algorithm , and RRSS our is the second fastest algorithm .
Second , with the help of Theorem 4 , RRSS our is highly faster than RRSS Nie , averagely obtaining a 559 times acceleration .
Third , ARSS is dramatically faster than :
Performances of TED , RRSS and ARSS : ( left - a ) speed in seconds , prediction accuracies .
In terms of speed , with the help of Theorem 4 , RRSSour is averagely 559 + times faster than the authorial algorithm , i.e. RRSSNie ; ARSS achieves surprisingly 23275 + times acceleration compared with RRSSNie .
Due to the more robust loss in the p -norm , the prediction accuracy of ARSS is highly encouraging .
Datasets
Speed 1 ' ARSS ( N * ) ' means the task of selecting samples from the whole dataset ( with N * samples as shown in the 2 nd column in ) , while ' TED ' to ' ARSS ' indicate the problem of dealing with the candidate sample sets ( with N samples as shown in the 3 rd column in ) .
verify an average acceleration of 23275 times faster than RRSS Nie and 281 times faster than TED .
This means that for example if it takes RRSS Nie 100 years to do a subset selection task , it only takes our method 1.6 days to address the same problem .
Finally , we apply ARSS to the whole sample set of each data .
RRSS Nie and TED ; the results in
The results are displayed in the 6 th column in , showing its capability to process very large datasets .
Prediction Accuracy
Accuracy comparison
We conduct experiments on ten benchmark datasets .
For each dataset , the top 200 representative samples are selected for training .
The prediction accuracies are reported in , including the results of two popular classifiers .
Three observations can be drawn from this table .
First , Linear SVM generally outperforms KNN .
Second , in general , our method performs the best ; fora few cases , our method achieves comparable results with the best performances .
Third , compared with TED , both RRSS and ARSS achieve an appreciable advantage .
The above analyses are better illustrated in the last row of .
These results demonstrate that the p loss in our model is well suited to select exemplars from the sample sets of various quality .
Prediction accuracies vs. increasing K To give a more detailed comparison , shows the prediction accuracies versus growing K ( the number of selected samples ) .
There are two rows and four columns of sub-figures .
The top row shows the results of KNN , and the bottom one shows results of SVM .
Each column gives the result on one dataset .
As we shall see , the prediction accuracies generally increase as K increases .
Such case is consistent with the common view that more training data will boost the prediction accuracy .
For each sub-figure , ARSS is generally among the best .
This case implies that our robust objective ( 5 ) via the p - norm is feasible to select subsets from the data of varying qualities .
Conclusion
To deal with tremendous data of varying quality , we propose an accelerated robust subset selection ( ARSS ) method .
The p - norm is exploited to enhance the robustness against both outlier samples and outlier features .
Although the resulted objective is complex to solve , we propose a highly efficient solver via two techniques : ALM and equivalent derivations .
Via them , we greatly reduce the computational complexity from ON 4 to ON 2 L .
Here feature length L is much smaller than data size N , i.e. L N .
Extensive results on ten benchmark datasets verify that our method not only runs 10,000 + times faster than the most related method , but also outperforms state of the art methods .
Moreover , we propose an accelerated solver to highly speedup Nie 's method , theoretically reducing the computational complexity from ON 4 to ON 2 L + N L 3 .
Empirically , our accelerated solver could achieve equal results and 500 + times acceleration compared with the authorial solver .
Limitation .
Our efficient algorithm build on the observation that the number of samples is generally larger than feature length , i.e. N > L. For the case of N ?
L , the acceleration will be inapparent .
